{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 Activation Map Reconstruction Cont 1\n",
    "\n",
    "## CONSTRUCT\n",
    "\n",
    "**Goal**\n",
    "\n",
    "This file combine 500 sample. \n",
    "\n",
    "### Before Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, re, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Ignore warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the cardiac_ml_tools.py script\n",
    "%run ../cardiac_challenge/notebooks/cardiac_ml_tools.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of file pairs: 16117\n",
      "Example of file pair:\n",
      "../cardiac_challenge/intracardiac_dataset/data_hearts_dd_0p2_geo_act_3_bcl/pECGData_hearts_dd_0p2_geo_act_3_bcl_bcl.1000.pattern.0.volunteer.v1.npy\n",
      "../cardiac_challenge/intracardiac_dataset/data_hearts_dd_0p2_geo_act_3_bcl/VmData_hearts_dd_0p2_geo_act_3_bcl_bcl.1000.pattern.0.volunteer.v1.npy\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data_dirs = []\n",
    "regex = r'data_hearts_dd_0p2*'\n",
    "DIR = '../cardiac_challenge/intracardiac_dataset/' # path to the intracardiac_dataset\n",
    "\n",
    "for x in os.listdir(DIR):\n",
    "    if re.match(regex, x):\n",
    "        data_dirs.append(DIR + x)\n",
    "file_pairs = read_data_dirs(data_dirs)\n",
    "print('Number of file pairs: {}'.format(len(file_pairs)))\n",
    "# example of file pair\n",
    "print(\"Example of file pair:\")\n",
    "print(\"{}\\n{}\".format(file_pairs[0][0], file_pairs[0][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.1 Data Processing\n",
    "\n",
    "#### 3.1.1 Combine 500 Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_pairs is a list where each element is a tuple containing the file paths for ECG data and activation time data\n",
    "num_samples = 500  # Number of samples to process\n",
    "num_timesteps = 500  # Each ECG data has 500 timesteps\n",
    "num_leads = 12  # Standard ECG leads count after processing\n",
    "\n",
    "# Initialize arrays to store combined data\n",
    "ECGData_500 = np.zeros((num_samples, num_timesteps * num_leads))  # Flattened array for 12 leads data\n",
    "ActTime_500 = np.zeros((num_samples, 75))  # Store 75 activation times per sample\n",
    "\n",
    "# Process each sample\n",
    "for i in range(num_samples):\n",
    "    # Load ECG data\n",
    "    pECGData = np.load(file_pairs[i][0])\n",
    "    pECGData = get_standard_leads(pECGData)  # Convert to 12 standard leads\n",
    "    ECGData_500[i, :] = pECGData.flatten()  # Flatten and store in the combined array\n",
    "\n",
    "    # Load activation time data\n",
    "    VmData = np.load(file_pairs[i][1])\n",
    "    ActTime = get_activation_time(VmData)\n",
    "    ActTime_500[i, :] = ActTime.flatten()  # Flatten the (75, 1) array to fit into (75,) array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory if it does not exist\n",
    "output_dir = '../combine_dataset'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save combined datasets to .npy format\n",
    "np.save(os.path.join(output_dir, 'ecg_data_500.npy'), ECGData_500)\n",
    "np.save(os.path.join(output_dir, 'active_time_500.npy'), ActTime_500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After combine, check them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECGData_500 shape: (500, 6000)\n",
      "ActTime_500 shape: (500, 75)\n"
     ]
    }
   ],
   "source": [
    "print(\"ECGData_500 shape: {}\".format(ECGData_500.shape))\n",
    "print(\"ActTime_500 shape: {}\".format(ActTime_500.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.01860286,  0.02571926,  0.0071164 , ..., -0.00043366,\n",
       "         -0.00047924, -0.00037289],\n",
       "        [ 0.0166689 ,  0.01879395,  0.00212505, ..., -0.0003504 ,\n",
       "         -0.00037333, -0.00018206],\n",
       "        [ 0.01997983,  0.0190023 , -0.00097753, ..., -0.00093712,\n",
       "         -0.00066594, -0.00049339],\n",
       "        ...,\n",
       "        [ 0.00144014,  0.00167206,  0.00023193, ..., -0.00081582,\n",
       "          0.00018729, -0.00010726],\n",
       "        [ 0.00210478,  0.00299183,  0.00088705, ..., -0.00049417,\n",
       "          0.00041288,  0.00049894],\n",
       "        [ 0.00081549,  0.00133864,  0.00052315, ..., -0.0008989 ,\n",
       "          0.00046388,  0.00015533]]),\n",
       " array([[18., 31., 22., ..., 17., 11.,  5.],\n",
       "        [12.,  3.,  8., ...,  4., 10., 16.],\n",
       "        [10.,  6.,  1., ..., 19., 12.,  5.],\n",
       "        ...,\n",
       "        [11.,  6.,  1., ..., 40., 34., 27.],\n",
       "        [14.,  8.,  2., ..., 33., 40., 45.],\n",
       "        [16., 14., 10., ..., 43., 37., 31.]]))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ECGData_500, ActTime_500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle indices\n",
    "indices = np.arange(ECGData_500.shape[0])\n",
    "shuffled_indices = shuffle(indices, random_state=42)\n",
    "\n",
    "# Define the split point\n",
    "split_ratio = 0.8  # 80% train, 20% test\n",
    "split_point = int(len(shuffled_indices) * split_ratio)\n",
    "\n",
    "# Split indices into training and test sets\n",
    "train_indices = shuffled_indices[:split_point]\n",
    "test_indices = shuffled_indices[split_point:]\n",
    "\n",
    "# Use indices to create training and test data\n",
    "X_train = ECGData_500[train_indices]\n",
    "y_train = ActTime_500[train_indices]\n",
    "X_test = ECGData_500[test_indices]\n",
    "y_test = ActTime_500[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 3.26309393e-03,  3.85096802e-03,  5.87874090e-04, ...,\n",
       "         -3.45405229e-04, -3.29793799e-04, -2.22777099e-04],\n",
       "        [ 1.60217814e-03,  1.64650948e-03,  4.43313400e-05, ...,\n",
       "         -8.20579544e-04,  9.82572556e-05, -5.18787444e-05],\n",
       "        [ 3.04071210e-03,  2.47169349e-03, -5.69018612e-04, ...,\n",
       "         -5.44660959e-04, -3.09872489e-04, -1.26951169e-04],\n",
       "        ...,\n",
       "        [ 1.92237300e-04, -7.01320100e-04, -8.93557400e-04, ...,\n",
       "         -1.82680515e-03,  6.56034148e-04,  1.21244099e-03],\n",
       "        [ 3.17828010e-03,  7.84831200e-03,  4.67003190e-03, ...,\n",
       "         -3.95781928e-04, -1.94673998e-04, -3.90945876e-05],\n",
       "        [ 7.00068570e-03,  7.81426910e-03,  8.13583400e-04, ...,\n",
       "         -4.71721327e-04, -7.82934897e-04, -8.88557697e-04]]),\n",
       " array([[12., 13., 18., ..., 24., 30., 36.],\n",
       "        [19., 13.,  4., ..., 32., 38., 44.],\n",
       "        [18., 11., 31., ..., 36., 31., 24.],\n",
       "        ...,\n",
       "        [64., 57., 48., ..., 42., 39., 33.],\n",
       "        [14., 20., 12., ..., 33., 28., 23.],\n",
       "        [41., 33., 31., ..., 29., 39., 49.]]),\n",
       " array([[-2.04351597e-03, -7.63284970e-04,  1.28023100e-03, ...,\n",
       "         -1.57521301e-03, -9.94710614e-04, -8.63371414e-04],\n",
       "        [ 2.42108750e-03,  4.38617410e-03,  1.96508660e-03, ...,\n",
       "         -7.38941570e-04, -7.82695870e-04, -4.95450570e-04],\n",
       "        [ 2.81036100e-04,  4.65120500e-04,  1.84084400e-04, ...,\n",
       "          2.37591612e-04,  8.55920352e-04,  4.84832712e-04],\n",
       "        ...,\n",
       "        [ 1.36982551e-03,  9.22702112e-04, -4.47123398e-04, ...,\n",
       "         -9.41077288e-04, -1.39626009e-03, -1.83608979e-03],\n",
       "        [ 1.80866073e-02,  2.51901215e-02,  7.10351420e-03, ...,\n",
       "         -2.63094631e-04, -5.11079051e-04, -4.75318161e-04],\n",
       "        [ 1.83445348e-03,  1.75124313e-03, -8.32103500e-05, ...,\n",
       "          4.05863189e-04, -5.42727751e-04, -7.00300851e-04]]),\n",
       " array([[20., 16., 11., ..., 42., 35., 28.],\n",
       "        [21., 16., 12., ..., 11.,  6.,  2.],\n",
       "        [72., 60., 49., ..., 43., 40., 34.],\n",
       "        ...,\n",
       "        [77., 81., 84., ..., 40., 46., 49.],\n",
       "        [18., 32., 23., ..., 18., 12.,  5.],\n",
       "        [55., 59., 45., ..., 29., 39., 45.]]))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((400, 6000), (400, 75), (100, 6000), (100, 75))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Modeling\n",
    "\n",
    "#### 3.2.1 Define the 1D CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple1DCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Simple1DCNN, self).__init__()\n",
    "\n",
    "        # convolutional layers -> relu -> convolutional layers -> relu -> pooling -> flatten -> fully connected layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=12, out_channels=16, kernel_size=3, padding=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(3)\n",
    "        self.flat = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(5376, 512)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # 32 channels * 1500 length\n",
    "        self.fc2 = nn.Linear(512, 75)  # Output the activation times\n",
    "        # # Layer 1: First convolutional layer\n",
    "        # self.conv1 = nn.Conv1d(in_channels=12, out_channels=16, kernel_size=5, padding=2)\n",
    "        # # Layer 2: ReLU activation function\n",
    "        # self.relu = nn.ReLU()\n",
    "        # # Layer 3: Max pooling layer\n",
    "        # self.pool = nn.MaxPool1d(2)\n",
    "        # # Layer 4: Second convolutional layer\n",
    "        # self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5, padding=2)\n",
    "        # # Layer 5: First fully connected layer\n",
    "        # self.fc1 = nn.Linear(32*1500, 100)\n",
    "        # # Layer 6: Second fully connected layer\n",
    "        # self.fc2 = nn.Linear(100, 75)  # Output the activation times\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        # print(x.size())  # This will print the shape [batch_size, num_channels, length]\n",
    "        # x = x.view(x.size(0), -1)  # Flatten the output for the dense layer\n",
    "        # print(x.size())  # This will print the flattened size\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Initialize the Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Simple1DCNN()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Configure Loss Function\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Prepare & Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1434.59912109375\n",
      "Epoch 2, Loss: 1424.7789306640625\n",
      "Epoch 3, Loss: 1411.237548828125\n",
      "Epoch 4, Loss: 1391.6141357421875\n",
      "Epoch 5, Loss: 1365.222900390625\n",
      "Epoch 6, Loss: 1331.3873291015625\n",
      "Epoch 7, Loss: 1289.508544921875\n",
      "Epoch 8, Loss: 1239.0467529296875\n",
      "Epoch 9, Loss: 1179.60205078125\n",
      "Epoch 10, Loss: 1110.9798583984375\n",
      "Epoch 11, Loss: 1033.2666015625\n",
      "Epoch 12, Loss: 947.083984375\n",
      "Epoch 13, Loss: 853.6998901367188\n",
      "Epoch 14, Loss: 755.3377075195312\n",
      "Epoch 15, Loss: 655.5318603515625\n",
      "Epoch 16, Loss: 559.5077514648438\n",
      "Epoch 17, Loss: 474.5154724121094\n",
      "Epoch 18, Loss: 409.87347412109375\n",
      "Epoch 19, Loss: 375.7651062011719\n",
      "Epoch 20, Loss: 378.8187561035156\n",
      "Epoch 21, Loss: 412.80914306640625\n",
      "Epoch 22, Loss: 453.71527099609375\n",
      "Epoch 23, Loss: 475.7220764160156\n",
      "Epoch 24, Loss: 469.61474609375\n",
      "Epoch 25, Loss: 441.6982116699219\n",
      "Epoch 26, Loss: 404.2726135253906\n",
      "Epoch 27, Loss: 368.5992431640625\n",
      "Epoch 28, Loss: 341.7756042480469\n",
      "Epoch 29, Loss: 326.31622314453125\n",
      "Epoch 30, Loss: 321.22210693359375\n",
      "Epoch 31, Loss: 323.5412292480469\n",
      "Epoch 32, Loss: 329.7457275390625\n",
      "Epoch 33, Loss: 336.6769714355469\n",
      "Epoch 34, Loss: 342.037841796875\n",
      "Epoch 35, Loss: 344.4957580566406\n",
      "Epoch 36, Loss: 343.560546875\n",
      "Epoch 37, Loss: 339.4175720214844\n",
      "Epoch 38, Loss: 332.7497863769531\n",
      "Epoch 39, Loss: 324.5509948730469\n",
      "Epoch 40, Loss: 316.0074462890625\n",
      "Epoch 41, Loss: 308.2902526855469\n",
      "Epoch 42, Loss: 302.3538818359375\n",
      "Epoch 43, Loss: 298.7827453613281\n",
      "Epoch 44, Loss: 297.568603515625\n",
      "Epoch 45, Loss: 298.0957946777344\n",
      "Epoch 46, Loss: 299.2498779296875\n",
      "Epoch 47, Loss: 299.7915344238281\n",
      "Epoch 48, Loss: 298.81610107421875\n",
      "Epoch 49, Loss: 296.1441650390625\n",
      "Epoch 50, Loss: 292.2870788574219\n"
     ]
    }
   ],
   "source": [
    "# Prepare\n",
    "X_train_tensor = torch.tensor(X_train.reshape(400, 12, 500), dtype=torch.float32)  # Add channel dimension\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "# Train\n",
    "num_epochs = 50  # or however many you deem necessary\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train_tensor)\n",
    "    loss = criterion(output, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Simple1DCNN(\n",
       "  (conv1): Conv1d(1, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (relu): ReLU()\n",
       "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(16, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (fc1): Linear(in_features=48000, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=100, out_features=75, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model\n",
    "model.eval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
